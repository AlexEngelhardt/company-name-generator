{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, TimeDistributed\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "from utils import generate_word\n",
    "from utils import text_to_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `corpus`, currently available choices are:\n",
    "\n",
    "- behemoth\n",
    "- black-speech\n",
    "- english\n",
    "- french\n",
    "- gallic\n",
    "- german\n",
    "- greek\n",
    "- latin\n",
    "- lorem-ipsum\n",
    "- pokemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paléozoïque\\n', 'tous\\n', 'électoraux\\n', 'palais\\n', 'gouvernementales\\n']\n"
     ]
    }
   ],
   "source": [
    "# Generate a list of words (including newline)\n",
    "corpus = \"french\"\n",
    "textfile = \"wordlists/\" + corpus + \".txt\"\n",
    "words = text_to_words(textfile)\n",
    "\n",
    "print(words[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3271 words\n",
      "\n",
      "vocabulary of 51 characters, including the \\n:\n",
      "['\\t', '\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', '²', 'à', 'â', 'ç', 'è', 'é', 'ê', 'í', 'î', 'ï', 'ô', 'ö', 'ù', 'û', 'œ', 'ί', 'α', 'γ', 'λ', 'τ', '—', '−']\n",
      "\n",
      "First two sample words:\n",
      "['paléozoïque\\n', 'tous\\n']\n"
     ]
    }
   ],
   "source": [
    "# Generate the set of unique characters (including newline)\n",
    "# https://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python\n",
    "chars = sorted(list(set([char for word in words for char in word])))\n",
    "\n",
    "VOCAB_SIZE = len(chars)\n",
    "N_WORDS = len(words)\n",
    "MAX_WORD_LEN = 12  # maximum company name length\n",
    "\n",
    "\n",
    "print(N_WORDS, \"words\\n\")\n",
    "print(\"vocabulary of\", len(chars), \"characters, including the \\\\n:\")\n",
    "print(chars)\n",
    "print(\"\\nFirst two sample words:\")\n",
    "print(words[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "char_to_ix = {char:ix for ix, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((N_WORDS, MAX_WORD_LEN, VOCAB_SIZE))\n",
    "Y = np.zeros((N_WORDS, MAX_WORD_LEN, VOCAB_SIZE))\n",
    "\n",
    "for word_i in range(N_WORDS):\n",
    "    word = words[word_i]\n",
    "    chars = list(word)\n",
    "    word_len = len(word)\n",
    "    \n",
    "    for char_j in range(min(len(word), MAX_WORD_LEN)):\n",
    "        char = chars[char_j]\n",
    "        char_ix = char_to_ix[char]\n",
    "        X[word_i, char_j, char_ix] = 1\n",
    "        if char_j > 0:\n",
    "            Y[word_i, char_j - 1, char_ix] = 1  # the 'next char' at time point char_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_NUM = 2\n",
    "HIDDEN_DIM = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 50)          20400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 50)          20200     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 51)          2601      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, 51)          0         \n",
      "=================================================================\n",
      "Total params: 43,201\n",
      "Trainable params: 43,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_scale(probs, temperature = 1.0):\n",
    "    # a low temperature (< 1 and approaching 0) results in the char sampling approaching the argmax.\n",
    "    # a high temperature (> 1, approaching infinity) results in sampling from a uniform distribution)\n",
    "    probs = np.exp(np.log(probs) / temperature)\n",
    "    probs = probs / np.sum(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(model, temperature = 1.0, min_word_length = 4):\n",
    "    X = np.zeros((1, MAX_WORD_LEN, VOCAB_SIZE))\n",
    "    \n",
    "    # sample the first character\n",
    "    initial_char_distribution = temp_scale(model.predict(X[:, 0:1, :]).flatten(), temperature)\n",
    "    \n",
    "    ix = 0\n",
    "    while ix == 0:  # make sure the initial character is not a newline (i.e. index 0)\n",
    "        ix = int(np.random.choice(VOCAB_SIZE, size = 1, p = initial_char_distribution))\n",
    "    \n",
    "    X[0, 0, ix] = 1\n",
    "    generated_word = [ix_to_char[ix].upper()]  # start with first character, then later successively append chars\n",
    "    \n",
    "    # sample all remaining characters\n",
    "    for i in range(1, MAX_WORD_LEN):\n",
    "        next_char_distribution = temp_scale(model.predict(X[:, 0:i, :])[:, i-1, :].flatten(), temperature)\n",
    "\n",
    "        \n",
    "        ix_choice = np.random.choice(VOCAB_SIZE, size = 1, p = next_char_distribution)\n",
    "        # ix_choice = np.argmax(next_char_distribution)  # <- corresponds to sampling with a very low temperature\n",
    "        ctr = 0\n",
    "        while ix_choice == 0 and i < min_word_length:\n",
    "            ctr += 1\n",
    "            # sample again if you picked the end-of-word token too early\n",
    "            ix_choice = np.random.choice(VOCAB_SIZE, size = 1, p = next_char_distribution)\n",
    "            if ctr > 1000:\n",
    "                print(\"caught in a near-infinite loop. You might have picked too low a temperature and the sampler just keeps sampling \\\\n's\")\n",
    "                break\n",
    "            \n",
    "        \n",
    "        next_ix = int(ix_choice)\n",
    "        X[0, i, next_ix] = 1\n",
    "        if next_ix == 0:\n",
    "            break\n",
    "        generated_word.append(ix_to_char[next_ix])\n",
    "    \n",
    "    return ('').join(generated_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"epoch \" + str(epoch) + \": \" + generate_word(model, temperature = 1.0, min_word_length = 4))\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end = on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: Tœ²iuraeil\n",
      "e\n",
      "epoch 50: Bey\n",
      "ne\n",
      "\n",
      "\n",
      "\n",
      "e\n",
      "\n",
      "epoch 100: Terne\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 150: Iul\n",
      "s\n",
      "\n",
      "\n",
      "tion\n",
      "epoch 200: Troupelété\n",
      "\n",
      "\n",
      "epoch 250: Victaines\n",
      "\n",
      "\n",
      "\n",
      "epoch 300: Finants\n",
      "oliè\n",
      "epoch 350: Obligète\n",
      "uen\n",
      "epoch 400: Listes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "b\n",
      "epoch 450: Apmoient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 31min 5s, sys: 2min 48s, total: 33min 53s\n",
      "Wall time: 10min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 64  # or: N_WORDS\n",
    "\n",
    "model.fit(X, Y, batch_size = BATCH_SIZE, verbose = 0, epochs = NUM_EPOCHS, callbacks = [print_callback])\n",
    "\n",
    "# save the model, but only if the h5 file doesn't exist yet:\n",
    "model_filename = \"models/\" + corpus + \"_\" + str(NUM_EPOCHS) + 'epochs.h5'\n",
    "if not os.path.isfile(model_filename):\n",
    "    model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indépendants\n",
      "Havonte\n",
      "\n",
      "\n",
      "\n",
      "e\n",
      "Réocyant\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Intellection\n",
      "Landes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t\n",
      "Limgentratio\n",
      "L\n",
      "gnonralism\n",
      "Contrôlés\n",
      "oi\n",
      "Libres\n",
      "\n",
      "\n",
      "\n",
      "e\n",
      "\n",
      "Lexe\n",
      "u\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Inactive\n",
      "\n",
      "\n",
      "b\n",
      "Légalise\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "b\n",
      "Longue\n",
      "o\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Législatif\n",
      "\n",
      "\n",
      "Remagueu\n",
      "t\n",
      "\n",
      "\n",
      "Lac\n",
      "aire\n",
      "\n",
      "e\n",
      "\n",
      "Andaquess\n",
      "\n",
      "\n",
      "\n",
      "Hémènen\n",
      "\n",
      "\n",
      "\n",
      "e\n",
      "Luge\n",
      "ourg\n",
      "\n",
      "\n",
      "\n",
      "Louverann\n",
      "\n",
      "\n",
      "\n",
      "Rage\n",
      "n\n",
      "\n",
      "\n",
      "tie\n",
      "Inonde\n",
      "te\n",
      "\n",
      "\n",
      "\n",
      "Gergnét\n",
      "ent\n",
      "\n",
      "Légaguan\n",
      "\n",
      "\n",
      "b\n",
      "Leurs\n",
      "e\n",
      "\n",
      "e\n",
      "\n",
      "\n",
      "Ff\n",
      "\n",
      "\n",
      "e\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Liltes\n",
      "obse\n",
      "\n",
      "Liandes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cutrus\n",
      "les\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a few words with the final model:\n",
    "\n",
    "for _ in range(30):\n",
    "    print(generate_word(model, temperature = 1, min_word_length = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
